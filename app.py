import sys
import importlib
importlib.reload(sys)
import threading

from faster_whisper import WhisperModel
import datetime
import gradio as gr
import os
import torch
import wave
import contextlib
import time
from configs.model_config import *
import nltk
from models.chatglm_llm import ChatGLM
# from langchain.document_loaders import UnstructuredFileLoader
from langchain.document_loaders import TextLoader

from textsplitter import ChineseTextSplitter
from tqdm import tqdm
from utils import torch_gc
import imageio
from wordcloud import WordCloud
import sys
from datetime import datetime
import ffmpeg
import numpy as np

nltk.data.path = [NLTK_DATA_PATH] + nltk.data.path

whisper_models = ["tiny", "base", "small", "medium", "medium.en", "large-v1", "large-v2"]

source_languages = {
    "è‹±æ–‡": "en",
    "ä¸­æ–‡": "zh"
}

source_language_list = [key[0] for key in source_languages.items()]

# æŠ½å–æ‘˜è¦çš„æç¤º
prompt_template = """ä¸ºä¸‹é¢çš„å†…å®¹ç”Ÿæˆä¸€ä»½ç²¾ç®€çš„æ‘˜è¦:


{text}


è¿”å›ä¸­æ–‡æ‘˜è¦å†…å®¹:"""

# ä½¿ç”¨refineæ¨¡å¼æŠ½å–æ‘˜è¦çš„æç¤º
refine_template = (
    "ä½ çš„å·¥ä½œæ˜¯ç”Ÿæˆä¸€ä»½å…¨æ–‡æ‘˜è¦.\n"
    "æˆ‘å·²ç»ä¸ºæŸä¸ªæ–‡æœ¬ç‰‡æ®µç”Ÿæˆäº†ä¸€ä»½æ‘˜è¦: {existing_answer}\n"
    "è¯·åœ¨ç»™å®šæ–°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æƒ…å†µä¸‹ç»§ç»­å®Œå–„è¿™ä»½æ‘˜è¦ã€‚\n"
    "------------\n"
    "{text}\n"
    "------------\n"
    ""
    "å¦‚æœè¿™æ®µæ–°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸èƒ½æä¾›é¢å¤–çš„ä¿¡æ¯,è¯·è¿”å›åŸå§‹çš„æ‘˜è¦"
)

def get_text_summary(txt_path):
    print("starting summarizing")
    # loader = UnstructuredFileLoader(txt_path, mode="elements")
    loader = TextLoader(txt_path, encoding="utf-8")
    textsplitter = ChineseTextSplitter(pdf=False, sentence_size=SENTENCE_SIZE)
    docs = loader.load_and_split(text_splitter=textsplitter)

    for i, line in enumerate(tqdm(docs)):
        torch_gc()
        if i == 0:
            summary = next(llm._call(prompt=prompt_template.replace("{text}", line.page_content), history=[], streaming=False))[0]
        else:
            summary = next(llm._call(prompt=refine_template.replace("{existing_answer}", summary).replace("{text}", line.page_content), history=[], streaming=False))[0]

    return summary

# åŠ è½½ChatGLMæ¨¡å‹
def load_chatglm():
    model_name = "THUDM/chatglm-6b-int8"
    print("æ­£åœ¨åŠ è½½æ¨¡å‹:" + model_name)
    llm = ChatGLM()
    llm.load_model(model_name_or_path=model_name, llm_device="cuda:0", use_ptuning_v2=False, use_lora=False)
    llm.temperature = 1e-3
    print(model_name + "æ¨¡å‹åŠ è½½å®Œæ¯•")
    return llm


for i in range(5):
    try:
        llm = load_chatglm()
        selected_source_lang = "ä¸­æ–‡"
        break
    except:
        print("åŠ è½½å¤±è´¥,æ­£åœ¨å°è¯•ç¬¬ " + str(i + 1) + "æ¬¡")
        time.sleep(5)


whisper_model = "medium"
for i in range(5):
    try:
        print("æ­£åœ¨åŠ è½½æ¨¡å‹:" + whisper_model)
        model = WhisperModel(whisper_model, device="cuda" if torch.cuda.is_available() else "cpu", compute_type="int8_float16")
        print(whisper_model + "æ¨¡å‹åŠ è½½å®Œæ¯•")
        break
    except Exception as e:
        print(whisper_model + "æ¨¡å‹åŠ è½½å¤±è´¥,æ­£åœ¨å°è¯•ç¬¬ " + str(i + 1) + "æ¬¡", e)
        time.sleep(5)


# ç”Ÿæˆå…³é”®è¯è¯äº‘å›¾
def get_wordcloud_pic(words_freq, **kwargs):
    bg_img = imageio.imread('./sources/{}.png'.format(kwargs['bg_name']))
    font_path = './sources/{}.ttf'.format(kwargs['font_type'])
    word_cloud = WordCloud(font_path=font_path, background_color=kwargs['color'], max_words=kwargs['top_k'], max_font_size=50, mask=bg_img)
    word_cloud.generate_from_frequencies(words_freq)
    word_cloud.to_file('./output/result.png')
    return imageio.imread('./output/result.png')

# æŠ½å–å…³é”®è¯
def extract_keyword(text):
    print("starting extracting keyword")
    keyword_extracation_prompt = f"ä½ æ‰®æ¼”çš„è§’è‰²æ˜¯å…³é”®è¯æŠ½å–å·¥å…·,è¯·ä»è¾“å…¥çš„æ–‡æœ¬ä¸­æŠ½å–å‡º10ä¸ªæœ€é‡è¦çš„å…³é”®è¯,å¤šä¸ªå…³é”®è¯ä¹‹é—´ç”¨å•ä¸ªé€—å·åˆ†å‰²: \n\n" + text
    print("æŠ½å–å†…å®¹ä¸º:", keyword_extracation_prompt)
    keyword_extracation_res = next(llm._call(prompt=keyword_extracation_prompt, history=[], streaming=False))[0]
    keyword_extracation_res = keyword_extracation_res.strip().replace("ï¼Œ", ",").replace("ï¼š", ":").strip("å…³é”®è¯").strip(":").strip("ã€‚")
    print("æŠ½å–çš„å…³é”®è¯ä¸º:", keyword_extracation_res)
    words = {}
    torch_gc()

    if "." in keyword_extracation_res:
        for r in keyword_extracation_res.split("\n"):
            if len(r) > 0:
                words[r[r.index(".") + 1:].strip()] = text.count(r[r.index(".") + 1:].strip())
    elif "," in keyword_extracation_res:
        for r in keyword_extracation_res.split(","):
            if len(r) > 0:
                words[r.strip()] = text.count(r.strip())
    elif "ã€" in keyword_extracation_res:
        for r in keyword_extracation_res.split("ã€"):
            if len(r) > 0:
                words[r.strip()] = text.count(r.strip())

    print("å…³é”®è¯è¯é¢‘ç»Ÿè®¡ç»“æœ:", words)
    return get_wordcloud_pic(words, color='white', top_k=51, bg_name='bg', font_type='wryh')

def extract_keyword_from_file(file_name):
    print("starting extracting keyword")
    f = open(file_name, 'r', encoding='utf-8')
    text = f.read().strip()
    keyword_extracation_prompt = f"ä½ æ‰®æ¼”çš„è§’è‰²æ˜¯å…³é”®è¯æŠ½å–å·¥å…·,è¯·ä»è¾“å…¥çš„æ–‡æœ¬ä¸­æŠ½å–å‡º10ä¸ªæœ€é‡è¦çš„å…³é”®è¯,å¤šä¸ªå…³é”®è¯ä¹‹é—´ç”¨å•ä¸ªé€—å·åˆ†å‰²: \n\n" + text
    f.close()
    print("æŠ½å–å†…å®¹ä¸º:", keyword_extracation_prompt)
    keyword_extracation_res = next(llm._call(prompt=keyword_extracation_prompt, history=[], streaming=False))[0]
    keyword_extracation_res = keyword_extracation_res.strip().replace("ï¼Œ", ",").replace("ï¼š", ":").strip("å…³é”®è¯").strip(":").strip("ã€‚")
    print("æŠ½å–çš„å…³é”®è¯ä¸º:", keyword_extracation_res)
    words = {}
    torch_gc()

    if "." in keyword_extracation_res:
        for r in keyword_extracation_res.split("\n"):
            if len(r) > 0:
                words[r[r.index(".") + 1:].strip()] = text.count(r[r.index(".") + 1:].strip())
    elif "," in keyword_extracation_res:
        for r in keyword_extracation_res.split(","):
            if len(r) > 0:
                words[r.strip()] = text.count(r.strip())
    elif "ã€" in keyword_extracation_res:
        for r in keyword_extracation_res.split("ã€"):
            if len(r) > 0:
                words[r.strip()] = text.count(r.strip())

    print("å…³é”®è¯è¯é¢‘ç»Ÿè®¡ç»“æœ:", words)
    return get_wordcloud_pic(words, color='white', top_k=51, bg_name='bg', font_type='wryh')


def speech_to_text(video_file_path):  # selected_source_lang, whisper_model):
    # for i in range(5):
    #     try:
    #         print("æ­£åœ¨åŠ è½½æ¨¡å‹:" + whisper_model)
    #         model = WhisperModel(whisper_model, device="cuda" if torch.cuda.is_available() else "cpu", compute_type="int8_float16")
    #         print(whisper_model + "æ¨¡å‹åŠ è½½å®Œæ¯•")
    #         break
    #     except Exception as e:
    #         print(whisper_model + "æ¨¡å‹åŠ è½½å¤±è´¥,æ­£åœ¨å°è¯•ç¬¬ " + str(i + 1) + "æ¬¡", e)
    #         time.sleep(5)

    if(video_file_path == None):
        raise ValueError("Error no video input")

    print("åŸå§‹è·¯å¾„:", video_file_path)

    try:
        filename, file_ending = os.path.splitext(f'{video_file_path}')
        new_video_file_path = filename + "_" + time.strftime("%Y%m%d%H%M%S", time.localtime()) + file_ending
        os.rename(video_file_path, new_video_file_path)
        print("æ–°çš„è·¯å¾„:", new_video_file_path)

        print(f'file enging is {file_ending}')
        audio_file = new_video_file_path.replace(file_ending, ".wav")
        print("starting conversion to wav")
        os.system(f'ffmpeg -i "{new_video_file_path}" -ar 16000 -ac 1 -c:a pcm_s16le "{audio_file}"')

        # Get duration
        with contextlib.closing(wave.open(audio_file,'r')) as f:
            frames = f.getnframes()
            rate = f.getframerate()
            duration = frames / float(rate)
        print(f"conversion to wav ready, duration of audio file: {duration}")

        options = dict(language=source_languages[selected_source_lang], beam_size=5, best_of=5)
        transcribe_options = dict(task="transcribe", **options)
        segments_raw, info = model.transcribe(audio_file, **transcribe_options)

        segments = []
        for segment_chunk in segments_raw:
            segments.append(segment_chunk.text)
        transcribe_text = " ".join(segments)
        print("transcribe audio done with fast whisper")

        output_txt_path = os.path.join("output", os.path.basename(new_video_file_path).split(".")[0] + ".txt")
        with open(output_txt_path, "w", encoding="utf-8") as wf:
            wf.write(transcribe_text)
            torch_gc()
            print("transcribe text writen into txt file")

        return transcribe_text, get_text_summary(output_txt_path), extract_keyword(transcribe_text)
    except Exception as e:
        raise RuntimeError(e)



SAMPLE_RATE = 16000

class RingBuffer:
    def __init__(self, size):
        self.size = size
        self.data = []
        self.full = False
        self.cur = 0

    def append(self, x):
        if self.size <= 0:
            return
        if self.full:
            self.data[self.cur] = x
            self.cur = (self.cur + 1) % self.size
        else:
            self.data.append(x)
            if len(self.data) == self.size:
                self.full = True

    def get_all(self):
        """ Get all elements in chronological order from oldest to newest. """
        all_data = []
        for i in range(len(self.data)):
            idx = (i + self.cur) % self.size
            all_data.append(self.data[idx])
        return all_data

    def has_repetition(self):
        prev = None
        for elem in self.data:
            if elem == prev:
                return True
            prev = elem
        return False

    def clear(self):
        self.data = []
        self.full = False
        self.cur = 0


def open_stream(stream, direct_url, preferred_quality):
    if direct_url:
        try:
            process = (
                ffmpeg.input(stream, loglevel="panic")
                .output("pipe:", format="s16le", acodec="pcm_s16le", ac=1, ar=SAMPLE_RATE)
                .run_async(pipe_stdout=True)
            )
        except ffmpeg.Error as e:
            raise RuntimeError(f"Failed to load audio: {e.stderr.decode()}") from e

        return process, None

    import streamlink
    import subprocess
    import threading
    stream_options = streamlink.streams(stream)
    if not stream_options:
        print("No playable streams found on this URL:", stream)
        sys.exit(0)

    option = None
    for quality in [preferred_quality, 'audio_only', 'audio_mp4a', 'audio_opus', 'best']:
        if quality in stream_options:
            option = quality
            break
    if option is None:
        # Fallback
        option = next(iter(stream_options.values()))

    def writer(streamlink_proc, ffmpeg_proc):
        while (not streamlink_proc.poll()) and (not ffmpeg_proc.poll()):
            try:
                chunk = streamlink_proc.stdout.read(1024)
                ffmpeg_proc.stdin.write(chunk)
            except (BrokenPipeError, OSError):
                pass

    cmd = ['streamlink', stream, option, "-O"]
    streamlink_process = subprocess.Popen(cmd, stdout=subprocess.PIPE)

    try:
        ffmpeg_process = (
            ffmpeg.input("pipe:", loglevel="panic")
            .output("pipe:", format="s16le", acodec="pcm_s16le", ac=1, ar=SAMPLE_RATE)
            .run_async(pipe_stdin=True, pipe_stdout=True)
        )
    except ffmpeg.Error as e:
        raise RuntimeError(f"Failed to load audio: {e.stderr.decode()}") from e

    thread = threading.Thread(target=writer, args=(streamlink_process, ffmpeg_process))
    thread.start()
    return ffmpeg_process, streamlink_process


def stream_video_translate(url, max_len=10, language=None, interval=5, history_buffer_size=0, preferred_quality="audio_only",
         use_vad=True, direct_url=False, faster_whisper_args=True, **decode_options):

    line_count = 0
    stream_video_file = f"output/stream_video_{time.strftime('%Y%m%d%H%M%S', time.localtime())}.txt"
    res_list = []
    this_str = ""
    n_bytes = interval * SAMPLE_RATE * 2  # Factor 2 comes from reading the int16 stream as bytes
    audio_buffer = RingBuffer((history_buffer_size // interval) + 1)
    previous_text = RingBuffer(history_buffer_size // interval)
    # å£°æ˜åŠ è½½å¥½çš„æ¨¡å‹
    global model

    if use_vad:
        from utils.vad import VAD
        vad = VAD()

    print("Opening stream...")
    ffmpeg_process, streamlink_process = open_stream(url, direct_url, preferred_quality)

    try:
        stream_summary, stream_keyword = None, None
        while ffmpeg_process.poll() is None:
            # Read audio from ffmpeg stream
            in_bytes = ffmpeg_process.stdout.read(n_bytes)
            if not in_bytes:
                break

            audio = np.frombuffer(in_bytes, np.int16).flatten().astype(np.float32) / 32768.0
            if use_vad and vad.no_speech(audio):
                print(f'{datetime.now().strftime("%H:%M:%S")}')
                continue
            audio_buffer.append(audio)

            # Decode the audio
            clear_buffers = False
            if faster_whisper_args:
                segments, info = model.transcribe(audio, language=language, **decode_options)

                decoded_language = "" if language else "(" + info.language + ")"
                decoded_text = ""
                previous_segment = ""
                for segment in segments:
                    if segment.text != previous_segment:
                        decoded_text += segment.text
                        previous_segment = segment.text

                new_prefix = decoded_text

            else:
                result = model.transcribe(np.concatenate(audio_buffer.get_all()),
                                          prefix="".join(previous_text.get_all()),
                                          language=language,
                                          without_timestamps=True,
                                          **decode_options)

                decoded_language = "" if language else "(" + result.get("language") + ")"
                decoded_text = result.get("text")
                new_prefix = ""
                for segment in result["segments"]:
                    if segment["temperature"] < 0.5 and segment["no_speech_prob"] < 0.6:
                        new_prefix += segment["text"]
                    else:
                        # Clear history if the translation is unreliable, otherwise prompting on this leads to
                        # repetition and getting stuck.
                        clear_buffers = True

            previous_text.append(new_prefix)

            if clear_buffers or previous_text.has_repetition():
                audio_buffer.clear()
                previous_text.clear()

            # æŠŠè½¬å†™çš„ç»“æœå†™å…¥æ–‡ä»¶
            with open(stream_video_file, "a+", encoding="utf-8") as f:
                context = f.read().strip() + " "
                context += decoded_text
                f.write(context)
                line_count += 1

            # ä¸è¦é¢‘ç¹çš„æ‘˜è¦ç”Ÿæˆå…³é”®è¯,å¤ªæµªè´¹æ—¶é—´,è¿™é‡Œåªæ˜¯ä¸ºäº†å°½å¿«å±•ç¤ºæ•ˆæœ
            if line_count % (max_len * 1) == 0:
                stream_summary = get_text_summary(stream_video_file)
                stream_keyword = extract_keyword_from_file(stream_video_file)

            tmp = f'{datetime.now().strftime("%H:%M:%S")} {decoded_language} {decoded_text}'
            length = len(res_list)
            if length >= max_len:
                res_list = res_list[length - max_len + 1:length]
            res_list.append(tmp)
            this_str = "\n".join(res_list)
            yield this_str, stream_summary, stream_keyword

        this_str += "\nStream ended"
        yield this_str, stream_summary, stream_keyword
    finally:
        ffmpeg_process.kill()
        if streamlink_process:
            streamlink_process.kill()

def reformat_freq(sr, y):
    """
    sample_rateä¸æ”¯æŒ48000ï¼Œè½¬æ¢ä¸º16000
    """
    if sr not in (
        48000,
        16000,
    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)
        raise ValueError("Unsupported rate", sr)
    if sr == 48000:
        y = (
            y
            .reshape((-1, 3))
            .mean(axis=1)
            .astype("int16")
        )
        sr = 16000
    return sr, y

res_list = []
microphone_file = f"output/microphone_{time.strftime('%Y%m%d%H%M%S', time.localtime())}.txt"

def microphone_translate(audio, stream_summary=None, stream_keyword=None, line_count=0, max_len=10, language=None, interval_sec=5, **decode_options):
    """å®æ—¶è½¬å½•éº¦å…‹é£è¾“å…¥è¯­éŸ³"""
    # å¼•ç”¨å…¨å±€å˜é‡ï¼Œä¹Ÿå¯ä»¥å¼•ç”¨stateå­˜å‚¨çŠ¶æ€ä¿¡æ¯æ¯”å¦‚stream_summaryï¼Œå› ä¸ºæµå¼è¾“å…¥å‡½æ•°å†…éƒ½æ˜¯ä¸´æ—¶å˜é‡ï¼Œä¸èƒ½åšçŠ¶æ€å»¶ç»­
    global model, res_list, microphone_file
    sample_rate, audio_stream = reformat_freq(*audio)
    # æ•°æ®è½¬æ¢ï¼Œæ¨¡å‹åªæ”¯æŒ16000é‡‡æ ·ç‡
    audio_stream = audio_stream.flatten().astype(np.float32) / 32768.0
    segments, info = model.transcribe(audio_stream, language=language, **decode_options)
    # æœ¬æ¬¡å¤„ç†çš„è½¬å½•æ–‡å­—
    decoded_text = ""
    previous_segment = ""
    for segment in segments:
        if segment.text != previous_segment:
            decoded_text += segment.text
            previous_segment = segment.text

    decoded_language = "" if language else "(" + info.language + ")"
    tmp = f'{datetime.now().strftime("%H:%M:%S")} {decoded_language} {decoded_text}'
    length = len(res_list)
    if length >= max_len:
        res_list = res_list[length - max_len + 1:length]
    # å¤šæ¬¡å¤„ç†çš„è½¬å½•æ–‡å­—
    res_list.append(tmp)
    stream = "\n".join(res_list)

    # æŠŠè½¬å†™çš„ç»“æœå†™å…¥æ–‡ä»¶
    with open(microphone_file, "a+", encoding="utf-8") as f:
        context = f.read().strip() + " "
        #context += stream
        context += decoded_text
        f.write(context)
        line_count += 1

    # ä¸è¦é¢‘ç¹çš„æ‘˜è¦ç”Ÿæˆå…³é”®è¯,å¤ªæµªè´¹æ—¶é—´,è¿™é‡Œåªæ˜¯ä¸ºäº†å°½å¿«å±•ç¤ºæ•ˆæœ
    if line_count % (max_len * 1) == 0:
        stream_summary = get_text_summary(microphone_file)
        stream_keyword = extract_keyword_from_file(microphone_file)

    # ä½¿ç”¨sleepæ§åˆ¶å•æ¬¡å¤„ç†çš„æ—¶é•¿æ¥æå‡è¯†åˆ«æ•ˆæœï¼Œå®Œå…¨å®æ—¶çš„æƒ…å†µï¼Œæ¨¡å‹ä¸èƒ½è”ç³»ä¸Šä¸‹æ–‡æ•ˆæœå¾ˆå·®
    time.sleep(interval_sec)
    # è¿”å›çŠ¶æ€
    return stream, stream_summary, stream_keyword, line_count

webui_title = """
# ğŸ‰ ChatGLM-Video-Sense+ ğŸ‰

é¡¹ç›®æ—¨åœ¨å°†ç›´æ’­è§†é¢‘å’Œè§†é¢‘æ–‡ä»¶è½¬å†™æˆæ–‡æœ¬,åœ¨æ–‡æœ¬æ‘˜è¦ä»¥åŠå…³é”®è¯æŠ½å–ä¸¤å¤§åŠŸèƒ½çš„åŠ æŒä¸‹,è¾…åŠ©ç”¨æˆ·å®ç°è§†é¢‘å†…å®¹æ™ºèƒ½æ„ŸçŸ¥

é¡¹ç›®åœ°å€ä¸º: [https://github.com/freeline55/ChatGLM-Video-Sense](https://github.com/freeline55/ChatGLM-Video-Sense)
"""


with gr.Blocks() as demo:
    gr.Markdown(webui_title)

    with gr.Tab("ç›´æ’­è§†é¢‘å®æ—¶è½¬å†™"):
        with gr.Row():
            with gr.Column():
                # äº¤äº’ç•Œé¢åŠèµ·
                url_input = gr.Textbox(label="è¾“å…¥urlåœ°å€")
                btn_stream = gr.Button("ç›´æ’­è½¬å†™")
                res_output = gr.Textbox(label="è½¬å†™ç»“æœ", lines=10, max_lines=15)

        with gr.Row():
            stream_text_summary = gr.Textbox(label="æ‘˜è¦ç»“æœ", lines=10, max_lines=20)
            stream_text_image = gr.Image(label="å…³é”®è¯è¯äº‘å›¾")

        btn_stream.click(stream_video_translate, inputs=url_input, outputs=[res_output, stream_text_summary, stream_text_image], queue=True)
    with gr.Tab("è§†é¢‘æ–‡ä»¶æ™ºèƒ½åˆ†æ"):
        with gr.Row():
            with gr.Column():
                video_in = gr.Video(label="éŸ³/è§†é¢‘æ–‡ä»¶", mirror_webcam=False, )
                # selected_source_lang = gr.Dropdown(choices=source_language_list, type="value", value="ä¸­æ–‡", label="è§†é¢‘è¯­ç§", interactive=True)
                # selected_whisper_model = gr.Dropdown(choices=whisper_models, type="value", value="medium", label="é€‰æ‹©æ¨¡å‹", interactive=True)
                btn_analyse = gr.Button("è§†é¢‘åˆ†æ")
        with gr.Row():
            text_translate = gr.Textbox(label="è½¬å†™ç»“æœ", lines=20, max_lines=50)
            text_summary = gr.Textbox(label="æ‘˜è¦ç»“æœ", lines=20, max_lines=50)
            text_image = gr.Image(label="å…³é”®è¯è¯äº‘å›¾")

        btn_analyse.click(
            speech_to_text,
            inputs=[video_in],
            # inputs=[video_in, selected_source_lang, selected_whisper_model],
            outputs=[text_translate, text_summary, text_image],
            queue=False
        )
    with gr.Tab("éº¦å…‹é£å®æ—¶è½¬å†™"):
        with gr.Row():
            with gr.Column():
                # äº¤äº’ç•Œé¢åŠèµ·
                mic_stream = gr.Audio(label="ç‚¹å‡»éº¦å…‹é£", source="microphone", type="numpy", streaming=True)
                line_count = gr.Number(label="ç´¯è®¡è¡Œæ•°", value=0)
                res_output = gr.Textbox(label="è½¬å†™ç»“æœ", lines=10, max_lines=15)

        with gr.Row():
            stream_text_summary = gr.Textbox(label="æ‘˜è¦ç»“æœ", lines=10, max_lines=20)
            stream_text_image = gr.Image(label="å…³é”®è¯è¯äº‘å›¾")
        # å®æ—¶æ›´æ–°stream_text_summary, stream_text_image
        mic_stream.stream(microphone_translate, inputs=[mic_stream, stream_text_summary, stream_text_image, line_count], outputs=[res_output, stream_text_summary, stream_text_image, line_count])

# å¯èƒ½æœ‰é—ç•™grè¿›ç¨‹ï¼Œå…³é—­æ‰€æœ‰grè¿›ç¨‹
gr.close_all()
time.sleep(3)
demo.queue().launch(server_name='0.0.0.0', share=False, inbrowser=False)
