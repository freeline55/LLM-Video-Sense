import sys
import importlib
importlib.reload(sys)
import datetime
import gradio as gr
import os
import torch
import time
import nltk
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from tqdm import tqdm
from utils import torch_gc
import imageio
from wordcloud import WordCloud
from datetime import datetime
import ffmpeg
import uuid
import numpy as np
from collections import defaultdict
from models.download_fasterwhisper import speech_to_text, whisper_model
from models.use_zhipu import get_qa

SENTENCE_SIZE = 512
SAMPLE_RATE = 16000
os.makedirs("output", exist_ok=True)
# è®¾ç½®ç¯å¢ƒå˜é‡
NLTK_DATA_PATH = os.path.join(os.path.dirname(__file__), "nltk_data")
nltk.data.path = [NLTK_DATA_PATH] + nltk.data.path


# æŠ½å–æ‘˜è¦çš„æç¤º
prompt_template = """ä¸ºä¸‹é¢çš„å†…å®¹ç”Ÿæˆä¸€ä»½ç²¾ç®€çš„æ‘˜è¦:


{text}


è¿”å›ä¸­æ–‡æ‘˜è¦å†…å®¹
"""

# ä½¿ç”¨refineæ¨¡å¼æŠ½å–æ‘˜è¦çš„æç¤º
refine_template = (
    "ä½ çš„å·¥ä½œæ˜¯ç”Ÿæˆä¸€ä»½å…¨æ–‡æ‘˜è¦.\n"
    "æˆ‘å·²ç»ä¸ºæŸä¸ªæ–‡æœ¬ç‰‡æ®µç”Ÿæˆäº†ä¸€ä»½æ‘˜è¦: {existing_answer}\n"
    "è¯·åœ¨ç»™å®šæ–°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æƒ…å†µä¸‹ç»§ç»­å®Œå–„è¿™ä»½æ‘˜è¦ã€‚\n"
    "------------\n"
    "{text}\n"
    "------------\n"
    ""
    "å¦‚æœè¿™æ®µæ–°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸èƒ½æä¾›é¢å¤–çš„ä¿¡æ¯,è¯·è¿”å›åŸå§‹çš„æ‘˜è¦"
)


# è·å–åˆ†å‰²åçš„æ–‡æœ¬
def get_split_docs(output_txt_path):
    # åŠ è½½å¹¶åˆ†å‰²è½¬å†™æ–‡æœ¬
    loader = TextLoader(output_txt_path, encoding="utf-8")
    pages = loader.load_and_split()
    text_splitter = CharacterTextSplitter(separator="\n", chunk_size=SENTENCE_SIZE, chunk_overlap=0)
    docs = text_splitter.split_documents(pages)
    return docs


# ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
def get_text_summary(output_txt_path):
    print("å¼€å§‹æ–‡æœ¬æ‘˜è¦")

    docs = get_split_docs(output_txt_path)
    for i, line in enumerate(tqdm(docs)):
        if i == 0:
            summary = get_qa(prompt_template.replace("{text}", line.page_content))
        else:
            summary = get_qa(refine_template.replace("{existing_answer}", summary).replace("{text}", line.page_content))

    return summary


# ç”Ÿæˆå…³é”®è¯è¯äº‘å›¾
def get_wordcloud_pic(words_freq, **kwargs):
    bg_img = imageio.imread('./sources/{}.png'.format(kwargs['bg_name']))
    font_path = './sources/{}.ttf'.format(kwargs['font_type'])
    word_cloud = WordCloud(font_path=font_path, background_color=kwargs['color'], max_words=kwargs['top_k'], max_font_size=50, mask=bg_img)
    word_cloud.generate_from_frequencies(words_freq)
    word_cloud.to_file('./output/result.png')
    return imageio.imread('./output/result.png')


# æŠ½å–å…³é”®è¯
def extract_keyword(output_txt_path):
    print("å¼€å§‹æŠ½å–å…³é”®è¯")

    docs = get_split_docs(output_txt_path)
    with open(output_txt_path, "r", encoding="utf-8") as f:
        text = f.read()

    words = {}
    for i, line in enumerate(tqdm(docs)):
        keyword_extracation_prompt = f"è¯·ä»è¾“å…¥çš„æ–‡æœ¬ä¸­æŠ½å–å‡ºåä¸ªæœ€é‡è¦çš„å…³é”®è¯,ç»“æœä½¿ç”¨é€—å·åˆ†éš”: \n{line.page_content}"
        keyword_extracation_res = get_qa(keyword_extracation_prompt).replace("ï¼Œ", ",").replace("ï¼š", ":").replace(":", "").strip("å…³é”®è¯").strip("ã€‚").strip()
        print("å…³é”®è¯æŠ½å–ç»“æœï¼š", keyword_extracation_res)
        if "." in keyword_extracation_res:
            for r in keyword_extracation_res.split("\n"):
                if len(r) > 0:
                    count = text.count(r[r.index(".") + 1:].strip())
                    if count > 0:
                        words[r[r.index(".") + 1:].strip()] = count
        elif "," in keyword_extracation_res:
            for r in keyword_extracation_res.split(","):
                if len(r) > 0:
                    count = text.count(r.strip())
                    if count > 0:
                        words[r.strip()] = count
        elif "ã€" in keyword_extracation_res:
            for r in keyword_extracation_res.split("ã€"):
                if len(r) > 0:
                    count = text.count(r.strip())
                    if count > 0:
                        words[r.strip()] = count

    print("å…³é”®è¯è¯é¢‘ç»Ÿè®¡ç»“æœ:", words)
    if len(words) > 0:
        return get_wordcloud_pic(words, color='white', top_k=51, bg_name='bg', font_type='wryh')


# ç¦»çº¿è§†é¢‘åˆ†æ
def offline_video_analyse(video_file_path):
    torch_gc()
    print("å¼€å§‹åˆ†æç¦»çº¿è§†é¢‘:", video_file_path)
    # è§†é¢‘è½¬æ–‡æœ¬
    file_prefix, transcribe_text = speech_to_text(video_file_path)

    # è½¬å†™æ–‡æœ¬ä¿å­˜
    output_txt_path = os.path.join("output",  file_prefix + ".txt")
    with open(output_txt_path, "w", encoding="utf-8") as wf:
        wf.write(transcribe_text)

    # è·å–è½¬å†™æ–‡æœ¬ï¼Œæ–‡æœ¬æ‘˜è¦å’Œå…³é”®è¯
    return transcribe_text, get_text_summary(output_txt_path), extract_keyword(output_txt_path)


class RingBuffer:
    def __init__(self, size):
        self.size = size
        self.data = []
        self.full = False
        self.cur = 0

    def append(self, x):
        if self.size <= 0:
            return
        if self.full:
            self.data[self.cur] = x
            self.cur = (self.cur + 1) % self.size
        else:
            self.data.append(x)
            if len(self.data) == self.size:
                self.full = True

    def get_all(self):
        """ Get all elements in chronological order from oldest to newest. """
        all_data = []
        for i in range(len(self.data)):
            idx = (i + self.cur) % self.size
            all_data.append(self.data[idx])
        return all_data

    def has_repetition(self):
        prev = None
        for elem in self.data:
            if elem == prev:
                return True
            prev = elem
        return False

    def clear(self):
        self.data = []
        self.full = False
        self.cur = 0


def open_stream(stream, direct_url, preferred_quality):
    if direct_url:
        try:
            process = (
                ffmpeg.input(stream, loglevel="panic")
                .output("pipe:", format="s16le", acodec="pcm_s16le", ac=1, ar=SAMPLE_RATE)
                .run_async(pipe_stdout=True)
            )
        except ffmpeg.Error as e:
            raise RuntimeError(f"Failed to load audio: {e.stderr.decode()}") from e

        return process, None

    import streamlink
    import subprocess
    import threading
    stream_options = streamlink.streams(stream)
    if not stream_options:
        print("No playable streams found on this URL:", stream)
        sys.exit(0)

    option = None
    for quality in [preferred_quality, 'audio_only', 'audio_mp4a', 'audio_opus', 'best']:
        if quality in stream_options:
            option = quality
            break
    if option is None:
        # Fallback
        option = next(iter(stream_options.values()))

    def writer(streamlink_proc, ffmpeg_proc):
        while (not streamlink_proc.poll()) and (not ffmpeg_proc.poll()):
            try:
                chunk = streamlink_proc.stdout.read(1024)
                ffmpeg_proc.stdin.write(chunk)
            except (BrokenPipeError, OSError):
                pass

    cmd = ['streamlink', stream, option, "-O"]
    streamlink_process = subprocess.Popen(cmd, stdout=subprocess.PIPE)

    try:
        ffmpeg_process = (
            ffmpeg.input("pipe:", loglevel="panic")
            .output("pipe:", format="s16le", acodec="pcm_s16le", ac=1, ar=SAMPLE_RATE)
            .run_async(pipe_stdin=True, pipe_stdout=True)
        )
    except ffmpeg.Error as e:
        raise RuntimeError(f"Failed to load audio: {e.stderr.decode()}") from e

    thread = threading.Thread(target=writer, args=(streamlink_process, ffmpeg_process))
    thread.start()
    return ffmpeg_process, streamlink_process


stream_status = True


def update_stream_status():
    global stream_status
    stream_status = False


def stream_video_translate(url, max_len=10, language=None, interval=5, history_buffer_size=0, preferred_quality="audio_only", use_vad=True, direct_url=False, faster_whisper_args=True, **decode_options):
    global stream_status

    stream_status = True
    line_count = 0
    stream_video_file = f"output/stream_video_{time.strftime('%Y%m%d%H%M%S', time.localtime())}.txt"
    res_list = []
    this_str = ""
    n_bytes = interval * SAMPLE_RATE * 2  # Factor 2 comes from reading the int16 stream as bytes
    audio_buffer = RingBuffer((history_buffer_size // interval) + 1)
    previous_text = RingBuffer(history_buffer_size // interval)

    if use_vad:
        from utils.vad import VAD
        vad = VAD()

    print("Opening stream...")
    ffmpeg_process, streamlink_process = open_stream(url, direct_url, preferred_quality)

    try:
        stream_summary, stream_keyword = None, None
        while ffmpeg_process.poll() is None and stream_status:
            # Read audio from ffmpeg stream
            in_bytes = ffmpeg_process.stdout.read(n_bytes)
            if not in_bytes:
                break

            torch_gc()
            audio = np.frombuffer(in_bytes, np.int16).flatten().astype(np.float32) / 32768.0
            if use_vad and vad.no_speech(audio):
                print(f'{datetime.now().strftime("%H:%M:%S")}')
                continue
            audio_buffer.append(audio)

            # Decode the audio
            clear_buffers = False
            segments, info = whisper_model.transcribe(audio, language=language, **decode_options)

            decoded_language = "" if language else "(" + info.language + ")"
            decoded_text = ""
            previous_segment = ""
            for segment in segments:
                if segment.text != previous_segment:
                    decoded_text += segment.text
                    previous_segment = segment.text

            new_prefix = decoded_text
            previous_text.append(new_prefix)

            if clear_buffers or previous_text.has_repetition():
                audio_buffer.clear()
                previous_text.clear()

            # æŠŠè½¬å†™çš„ç»“æœå†™å…¥æ–‡ä»¶
            with open(stream_video_file, "a+", encoding="utf-8") as f:
                context = f.read().strip() + " "
                context += decoded_text
                f.write(context)
                line_count += 1

            # ä¸è¦é¢‘ç¹çš„æ‘˜è¦ç”Ÿæˆå…³é”®è¯,å¤ªæµªè´¹æ—¶é—´,è¿™é‡Œåªæ˜¯ä¸ºäº†å°½å¿«å±•ç¤ºæ•ˆæœ
            if line_count % (max_len * 1) == 0:
                stream_summary = get_text_summary(stream_video_file)
                stream_keyword = extract_keyword(stream_video_file)

            tmp = f'{datetime.now().strftime("%H:%M:%S")} {decoded_language} {decoded_text}'
            print(tmp)

            length = len(res_list)
            if length >= max_len:
                res_list = res_list[length - max_len + 1:length]
            res_list.append(tmp)
            this_str = "\n".join(res_list)
            yield this_str, stream_summary, stream_keyword

        this_str += "\nStream ended"
        yield this_str, stream_summary, stream_keyword
    finally:
        ffmpeg_process.kill()
        if streamlink_process:
            streamlink_process.kill()


def reformat_freq(sr, y):
    """
    sample_rateä¸æ”¯æŒ48000ï¼Œè½¬æ¢ä¸º16000
    """
    if sr not in (
        48000,
        16000,
    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)
        raise ValueError("Unsupported rate", sr)
    if sr == 48000:
        y = (
            y
            .reshape((-1, 3))
            .mean(axis=1)
            .astype("int16")
        )
        sr = 16000
    return sr, y


mic_dicts = defaultdict(dict)


def get_summary_keyword(key):
    if key not in mic_dicts:
        return None, None

    return get_text_summary(mic_dicts[key]["filename"]), extract_keyword(mic_dicts[key]["filename"])


def microphone_translate(audio, key, language=None, interval_sec=5, **decode_options):
    if key is None or len(key) <= 0:
        key = ''.join(str(uuid.uuid4()).split('-'))
        filename = f"output/microphone_{time.strftime('%Y%m%d%H%M%S', time.localtime())}.txt"
        mic_dicts[key] = {"line_count": 0,  "res_list": [], "filename": filename}

    torch_gc()
    """å®æ—¶è½¬å½•éº¦å…‹é£è¾“å…¥è¯­éŸ³"""
    # å¼•ç”¨å…¨å±€å˜é‡ï¼Œä¹Ÿå¯ä»¥å¼•ç”¨stateå­˜å‚¨çŠ¶æ€ä¿¡æ¯æ¯”å¦‚stream_summaryï¼Œå› ä¸ºæµå¼è¾“å…¥å‡½æ•°å†…éƒ½æ˜¯ä¸´æ—¶å˜é‡ï¼Œä¸èƒ½åšçŠ¶æ€å»¶ç»­
    sample_rate, audio_stream = reformat_freq(*audio)
    # æ•°æ®è½¬æ¢ï¼Œæ¨¡å‹åªæ”¯æŒ16000é‡‡æ ·ç‡
    audio_stream = audio_stream.flatten().astype(np.float32) / 32768.0
    segments, info = whisper_model.transcribe(audio_stream, language=language, **decode_options)
    # æœ¬æ¬¡å¤„ç†çš„è½¬å½•æ–‡å­—
    decoded_text = ""
    previous_segment = ""
    for segment in segments:
        if segment.text != previous_segment:
            decoded_text += segment.text
            previous_segment = segment.text

    decoded_language = "" if language else "(" + info.language + ")"
    tmp = f'{datetime.now().strftime("%H:%M:%S")} {decoded_language} {decoded_text}'

    # å¤šæ¬¡å¤„ç†çš„è½¬å½•æ–‡å­—
    mic_dicts[key]["res_list"].append(tmp)

    # æŠŠè½¬å†™çš„ç»“æœå†™å…¥æ–‡ä»¶
    with open(mic_dicts[key]["filename"], "a+", encoding="utf-8") as f:
        context = f.read().strip() + " "
        context += decoded_text
        f.write(context)
        mic_dicts[key]["line_count"] += 1

    # ä½¿ç”¨sleepæ§åˆ¶å•æ¬¡å¤„ç†çš„æ—¶é•¿æ¥æå‡è¯†åˆ«æ•ˆæœï¼Œå®Œå…¨å®æ—¶çš„æƒ…å†µï¼Œæ¨¡å‹ä¸èƒ½è”ç³»ä¸Šä¸‹æ–‡æ•ˆæœå¾ˆå·®
    time.sleep(interval_sec)

    # è¿”å›çŠ¶æ€
    return "\n".join(mic_dicts[key]["res_list"]), key


webui_title = """
# ğŸ‰ è§†é¢‘å†…å®¹æ™ºèƒ½æ„ŸçŸ¥ ğŸ‰

é¡¹ç›®æ—¨åœ¨å°†ç›´æ’­è§†é¢‘ã€è§†é¢‘æ–‡ä»¶å’Œå®æ—¶éŸ³é¢‘è½¬å†™æˆæ–‡æœ¬ï¼Œåœ¨æ–‡æœ¬æ‘˜è¦ä»¥åŠå…³é”®è¯æŠ½å–ä¸¤å¤§åŠŸèƒ½çš„åŠ æŒä¸‹ï¼Œè¾…åŠ©ç”¨æˆ·å¿«é€Ÿè·å–éŸ³é¢‘å’Œè§†é¢‘çš„æ ¸å¿ƒå†…å®¹ï¼Œæé«˜å­¦ä¹ å’Œå·¥ä½œæ•ˆç‡

"""


with gr.Blocks() as demo:
    gr.Markdown(webui_title)

    with gr.Tab("ç›´æ’­è§†é¢‘åœ¨çº¿åˆ†æ"):
        with gr.Row():
            with gr.Column():
                # äº¤äº’ç•Œé¢åŠèµ·
                url_input = gr.Textbox(label="è¾“å…¥urlåœ°å€")
                with gr.Row():
                    btn_stream = gr.Button("ç›´æ’­è½¬å†™")
                    btn_stop = gr.Button("åœæ­¢è½¬å†™")
                res_output = gr.Textbox(label="è½¬å†™ç»“æœ", lines=10, max_lines=15)

        with gr.Row():
            stream_text_summary = gr.Textbox(label="æ‘˜è¦ç»“æœ", lines=10, max_lines=20)
            stream_text_image = gr.Image(label="å…³é”®è¯è¯äº‘å›¾")

        btn_stream.click(stream_video_translate, inputs=url_input, outputs=[res_output, stream_text_summary, stream_text_image], queue=True)
        btn_stop.click(update_stream_status)
    with gr.Tab("è§†é¢‘æ–‡ä»¶åœ¨çº¿åˆ†æ"):
        with gr.Row():
            with gr.Column():
                video_in = gr.Video(label="éŸ³/è§†é¢‘æ–‡ä»¶", mirror_webcam=False)
                btn_analyse = gr.Button("è§†é¢‘åˆ†æ")
        with gr.Row():
            text_translate = gr.Textbox(label="è½¬å†™ç»“æœ", lines=20, max_lines=50)
            text_summary = gr.Textbox(label="æ‘˜è¦ç»“æœ", lines=20, max_lines=50)
            text_image = gr.Image(label="å…³é”®è¯è¯äº‘å›¾")

        btn_analyse.click(
            offline_video_analyse,
            inputs=[video_in],
            outputs=[text_translate, text_summary, text_image],
            queue=False
        )
    with gr.Tab("å®æ—¶éŸ³é¢‘åœ¨çº¿åˆ†æ"):
        with gr.Row():
            with gr.Column():
                # äº¤äº’ç•Œé¢åŠèµ·
                mic_stream = gr.Audio(label="ç‚¹å‡»éº¦å…‹é£", source="microphone", type="numpy", streaming=True)
                btn_summary_keyword = gr.Button("ç”Ÿæˆæ‘˜è¦å’Œå…³é”®è¯")
                key = gr.Textbox(label="key", lines=1, max_lines=1, interactive=False, visible=False)
                res_output = gr.Textbox(label="è½¬å†™ç»“æœ", lines=10, max_lines=15)

        with gr.Row():
            stream_text_summary = gr.Textbox(label="æ‘˜è¦ç»“æœ", lines=10, max_lines=20)
            stream_text_image = gr.Image(label="å…³é”®è¯è¯äº‘å›¾")

        btn_summary_keyword.click(get_summary_keyword, inputs=key, outputs=[stream_text_summary, stream_text_image])
        mic_stream.stream(microphone_translate, inputs=[mic_stream, key], outputs=[res_output, key])

# å¯èƒ½æœ‰é—ç•™grè¿›ç¨‹ï¼Œå…³é—­æ‰€æœ‰grè¿›ç¨‹
gr.close_all()
time.sleep(3)
demo.queue().launch(server_name='0.0.0.0', server_port=7860, share=False, inbrowser=False)





